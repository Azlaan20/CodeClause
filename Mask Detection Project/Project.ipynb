{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # **Project Name:** Mask Detection Project #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **Step 1:** Importing Required Libraries ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Import of the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **Step 2:** Data Preprocessing ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the data preprocessing step, we import the data from the respective directory and divide the image data into three respective categories. The image data is associated with the given label categories and then resized and shuffled. This data is then split into training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'D:\\Online Courses\\CodeClause Internship\\Mask Detection Project\\Dataset'\n",
    "categories = ['mask_weared_incorrect', 'with_mask', 'without_mask']\n",
    "img_size = (100, 100)  # Adjust image size as needed\n",
    "\n",
    "data = []\n",
    "\n",
    "for category in categories:\n",
    "    folder = os.path.join(data_dir, category)\n",
    "    label = categories.index(category)\n",
    "    \n",
    "    for img_name in os.listdir(folder):\n",
    "        img_path = os.path.join(folder, img_name)\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.resize(img, img_size)\n",
    "        data.append([img, label])\n",
    "\n",
    "# Shuffle the data\n",
    "np.random.shuffle(data)\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for img, label in data:\n",
    "    X.append(img)\n",
    "    y.append(label)\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Normalize pixel values\n",
    "X = X / 255.0\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **Step 3:** Model Design ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I designed a CNN model which has a very basic architecture. The model consists of 3 convolutional layers and 3 max pooling layers. The model is then flattened and passed through a dense layer. The model is then compiled using the Adam optimizer and categorical cross entropy loss function. It meets our requirements adeqately. The model is then trained on the training data and validated on the validation data. The model is then saved as a .h5 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "model = models.Sequential()\n",
    "\n",
    "# Convolutional layers\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(100, 100, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# Flatten layer\n",
    "model.add(layers.Flatten())\n",
    "\n",
    "# Fully connected layers\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dense(3, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "225/225 [==============================] - 53s 226ms/step - loss: 0.3370 - accuracy: 0.8640 - val_loss: 0.2128 - val_accuracy: 0.9238\n",
      "Epoch 2/10\n",
      "225/225 [==============================] - 49s 218ms/step - loss: 0.1554 - accuracy: 0.9456 - val_loss: 0.1379 - val_accuracy: 0.9549\n",
      "Epoch 3/10\n",
      "225/225 [==============================] - 52s 229ms/step - loss: 0.0899 - accuracy: 0.9699 - val_loss: 0.0798 - val_accuracy: 0.9716\n",
      "Epoch 4/10\n",
      "225/225 [==============================] - 50s 224ms/step - loss: 0.0722 - accuracy: 0.9772 - val_loss: 0.0950 - val_accuracy: 0.9661\n",
      "Epoch 5/10\n",
      "225/225 [==============================] - 48s 215ms/step - loss: 0.0462 - accuracy: 0.9846 - val_loss: 0.0792 - val_accuracy: 0.9733\n",
      "Epoch 6/10\n",
      "225/225 [==============================] - 49s 219ms/step - loss: 0.0319 - accuracy: 0.9905 - val_loss: 0.1012 - val_accuracy: 0.9627\n",
      "Epoch 7/10\n",
      "225/225 [==============================] - 48s 215ms/step - loss: 0.0331 - accuracy: 0.9887 - val_loss: 0.0515 - val_accuracy: 0.9850\n",
      "Epoch 8/10\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.0313 - accuracy: 0.9897 - val_loss: 0.0561 - val_accuracy: 0.9839\n",
      "Epoch 9/10\n",
      "225/225 [==============================] - 49s 216ms/step - loss: 0.0162 - accuracy: 0.9943 - val_loss: 0.0581 - val_accuracy: 0.9866\n",
      "Epoch 10/10\n",
      "225/225 [==============================] - 49s 216ms/step - loss: 0.0260 - accuracy: 0.9911 - val_loss: 0.0587 - val_accuracy: 0.9833\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2334df4df50>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 - 3s - loss: 0.0587 - accuracy: 0.9833 - 3s/epoch - 45ms/step\n",
      "Test accuracy: 98.33%\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=2)\n",
    "print(f'Test accuracy: {test_accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Azlaan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model.save('mask_detection_model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
